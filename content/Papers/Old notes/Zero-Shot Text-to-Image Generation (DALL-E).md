---
created: 2023-07-24
updated: 2023-07-24
---
>[!info]  
> **Year**:: 2021
> **Title**: Zero-Shot Text-to-Image Generation
> **Authors**: Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever
>**Journal**:: *arXiv:2102.12092 [cs]*   
> **URL**: http://arxiv.org/abs/2102.12092
> **Status**:: Want to Read
> **Stars**::
> **Tags**:


> [!Abstract]  
> Text-to-image generation has traditionally focused on ﬁnding better modeling assumptions for training on a ﬁxed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufﬁcient data and scale, our approach is competitive with previous domain-speciﬁc models when evaluated in a zero-shot fashion.  

> [!Quick Summary]  
>**Summary**::



%% Import Date: 2023-07-24T22:54:54.711+01:00 %%
