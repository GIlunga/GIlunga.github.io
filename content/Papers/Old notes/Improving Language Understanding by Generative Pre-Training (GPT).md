---
created: 2023-07-24
updated: 2023-07-24
---
>[!info]  
> **Year**:: 2018
> **Title**: Improving Language Understanding by Generative Pre-Training
> **Authors**: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever
>**Journal**:: **   
> **URL**: 
> **Status**:: Want to Read
> **Stars**::
> **Tags**:


> [!Abstract]  
> Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).  

> [!Quick Summary]  
>**Summary**::



%% Import Date: 2023-07-25T22:19:33.984+01:00 %%
