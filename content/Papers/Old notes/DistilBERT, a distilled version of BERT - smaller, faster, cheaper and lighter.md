---
created: 2023-07-24
updated: 2023-07-24
---
>[!info]  
> **Year**:: 2020
> **Title**: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
> **Authors**: Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf
>   
> **URL**: http://arxiv.org/abs/1910.01108
> **Status**:: Want to Read
> **Stars**::
> **Tags**:


> [!Abstract]  
> As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-speciﬁc models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.  

> [!Quick Summary]  
>**Summary**::



%% Import Date: 2023-07-24T22:42:25.050+01:00 %%
