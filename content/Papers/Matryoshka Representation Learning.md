---
date: 2024-01-27
tags:
  - "#dim_reduction"
  - year_2022
  - NeurIPS
  - wip
publish: "false"
aliases:
  - matryoshka
---
The paper introduces a new method for training embeddings which encodes information at different granularities. The first dimensions of the embedding can be used when needed to reduce computation cost. This flexible representation performs at least as well as directly learning a small embedding but allows multiple embedding sizes.

- focused on classification
- and retrieval
- 


>[!info]  
> **Paper link**: https://arxiv.org/abs/2205.13147
> **Code link**: https://github.com/RAIVNLab/MRL

# Introduction

# Deep dive
## What is goal of the paper?
- Flexible representations
- Why does it matter?
	- faster inference
- What is the significance/impact of the conclusion?
	- Simple change to training
	- 

## What is the approach?
- Is the approach well-motivated given existing literature?
- Code

## Results
- Are they correct?
- Are they rigorous?
- What else could be done?
- Datasets + experiments
- Evaluation metrics

## Next steps
- Where do we go from here?